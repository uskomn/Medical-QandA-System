import json
import re
from typing import List, Dict, Any, Tuple
from neo4j import GraphDatabase
import requests
import os

class KnowledgeGraphRetrieval:
    """çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸æ¨ç†ç³»ç»Ÿ"""

    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str,
                 deepseek_api_key: str = None):
        """
        åˆå§‹åŒ–
        Args:
            neo4j_uri: Neo4jæ•°æ®åº“URI
            neo4j_user: ç”¨æˆ·å
            neo4j_password: å¯†ç 
            deepseek_api_key: DeepSeek APIå¯†é’¥
        """
        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))
        self.deepseek_api_key = deepseek_api_key or os.getenv('DEEPSEEK_API_KEY')
        self.deepseek_api_url = "https://api.deepseek.com/v1/chat/completions"

    def close(self):
        """å…³é—­è¿æ¥"""
        self.driver.close()

    # ========== 1. å›¾æ£€ç´¢æ¨¡å— ==========

    def retrieve_relevant_subgraph(self, query: str, max_depth: int = 2,
                                   top_k: int = 10) -> Dict[str, Any]:
        """
        æ£€ç´¢ç›¸å…³å­å›¾
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            max_depth: æœ€å¤§æ£€ç´¢æ·±åº¦
            top_k: è¿”å›top-kä¸ªæœ€ç›¸å…³çš„è·¯å¾„
        Returns:
            å­å›¾æ•°æ®
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ” å¼€å§‹æ£€ç´¢ç›¸å…³å­å›¾")
        print(f"æŸ¥è¯¢: {query}")
        print(f"{'=' * 60}\n")

        # 1. æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“
        entities = self._extract_entities_from_query(query)
        print(f"âœ“ æå–åˆ°å…³é”®å®ä½“: {entities}\n")

        # 2. åœ¨å›¾è°±ä¸­æŸ¥æ‰¾åŒ¹é…çš„èŠ‚ç‚¹
        matched_nodes = self._find_matching_nodes(entities)
        print(f"âœ“ åŒ¹é…åˆ° {len(matched_nodes)} ä¸ªå›¾è°±èŠ‚ç‚¹\n")

        if not matched_nodes:
            print("âœ— æœªæ‰¾åˆ°åŒ¹é…èŠ‚ç‚¹\n")
            return {"nodes": [], "relationships": [], "paths": []}

        # 3. æ‰©å±•å­å›¾(BFS)
        subgraph = self._expand_subgraph(matched_nodes, max_depth, top_k)
        print(f"âœ“ æ‰©å±•å­å›¾å®Œæˆ:")
        print(f"  - èŠ‚ç‚¹æ•°: {len(subgraph['nodes'])}")
        print(f"  - å…³ç³»æ•°: {len(subgraph['relationships'])}")
        print(f"  - è·¯å¾„æ•°: {len(subgraph['paths'])}\n")

        return subgraph

    def _extract_entities_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®å®ä½“"""
        prompt = f"""ä»ä»¥ä¸‹åŒ»ç–—é—®é¢˜ä¸­æå–å…³é”®å®ä½“(ç–¾ç—…ã€æ²»ç–—ã€è¯ç‰©ã€æ£€æŸ¥ç­‰)ã€‚

é—®é¢˜: {query}

åªè¿”å›JSONæ•°ç»„,æ ¼å¼: ["å®ä½“1", "å®ä½“2", ...]

ç¤ºä¾‹:
é—®é¢˜: å¿ƒè„éª¤åœåº”è¯¥å¦‚ä½•æ€¥æ•‘?
è¾“å‡º: ["å¿ƒè„éª¤åœ", "æ€¥æ•‘"]
"""

        try:
            response = self._call_deepseek(prompt, max_tokens=200, temperature=0)
            response = response.strip()

            # æ¸…ç†markdown
            response = re.sub(r'```json\s*', '', response)
            response = re.sub(r'```\s*', '', response)

            entities = json.loads(response)
            return entities if isinstance(entities, list) else []
        except:
            # å¦‚æœå¤±è´¥,ä½¿ç”¨ç®€å•çš„å…³é”®è¯æå–
            keywords = []
            with self.driver.session() as session:
                # æŸ¥æ‰¾æŸ¥è¯¢ä¸­æåˆ°çš„æ‰€æœ‰èŠ‚ç‚¹åç§°
                result = session.run("""
                    MATCH (n)
                    WHERE $query CONTAINS n.name
                    RETURN DISTINCT n.name as name
                    LIMIT 10
                """, query=query)
                keywords = [record['name'] for record in result]

            return keywords if keywords else [query]

    def _find_matching_nodes(self, entities: List[str]) -> List[Dict]:
        """æŸ¥æ‰¾åŒ¹é…çš„å›¾è°±èŠ‚ç‚¹"""
        matched = []

        with self.driver.session() as session:
            for entity in entities:
                # æ¨¡ç³ŠåŒ¹é…
                result = session.run("""
                    MATCH (n)
                    WHERE n.name CONTAINS $entity
                    RETURN id(n) as node_id, 
                           labels(n)[0] as type,
                           n.name as name,
                           properties(n) as properties
                    LIMIT 5
                """, entity=entity)

                for record in result:
                    matched.append({
                        'id': record['node_id'],
                        'type': record['type'],
                        'name': record['name'],
                        'properties': dict(record['properties'])
                    })

        return matched

    def _expand_subgraph(self, seed_nodes: List[Dict], max_depth: int,
                         top_k: int) -> Dict[str, Any]:
        """æ‰©å±•å­å›¾"""
        node_ids = [node['id'] for node in seed_nodes]

        with self.driver.session() as session:
            # æŸ¥è¯¢å­å›¾è·¯å¾„
            result = session.run(f"""
                MATCH path = (start)-[*1..{max_depth}]-(end)
                WHERE id(start) IN $node_ids
                WITH path, 
                     length(path) as path_length,
                     [rel in relationships(path) | type(rel)] as rel_types
                RETURN path,
                       [node in nodes(path) | {{
                           id: id(node),
                           type: labels(node)[0],
                           name: node.name,
                           properties: properties(node)
                       }}] as nodes,
                       [rel in relationships(path) | {{
                           type: type(rel),
                           properties: properties(rel)
                       }}] as relationships,
                       path_length,
                       rel_types
                ORDER BY path_length ASC
                LIMIT $top_k
            """, node_ids=node_ids, top_k=top_k)

            # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
            all_nodes = {}
            all_relationships = []
            all_paths = []

            for record in result:
                nodes = record['nodes']
                rels = record['relationships']

                # æ”¶é›†èŠ‚ç‚¹
                for node in nodes:
                    node_id = node['id']
                    if node_id not in all_nodes:
                        all_nodes[node_id] = node

                # æ”¶é›†å…³ç³»
                for i, rel in enumerate(rels):
                    rel_data = {
                        'from': nodes[i]['id'],
                        'from_name': nodes[i]['name'],
                        'to': nodes[i + 1]['id'],
                        'to_name': nodes[i + 1]['name'],
                        'type': rel['type'],
                        'properties': rel['properties']
                    }
                    all_relationships.append(rel_data)

                # è®°å½•è·¯å¾„
                path_desc = ' -> '.join([
                                            f"{nodes[i]['name']}[{rels[i]['type']}]"
                                            for i in range(len(rels))
                                        ] + [nodes[-1]['name']])

                all_paths.append({
                    'nodes': nodes,
                    'relationships': rels,
                    'description': path_desc,
                    'length': record['path_length']
                })

            return {
                'nodes': list(all_nodes.values()),
                'relationships': all_relationships,
                'paths': all_paths
            }

    # ========== 2. è‡ªä¸€è‡´æ€§æ£€ç´¢ (æ”¹è¿›ç‰ˆ) ==========

    def self_consistency_retrieval(self, query: str, num_samples: int = 3) -> Dict[str, Any]:
        """
        è‡ªä¸€è‡´æ€§æ£€ç´¢: å¤šæ¬¡æ£€ç´¢å–ä¸€è‡´ç»“æœ,æ„å»ºé«˜ç½®ä¿¡åº¦å­å›¾
        Args:
            query: æŸ¥è¯¢
            num_samples: é‡‡æ ·æ¬¡æ•°
        Returns:
            é«˜ä¸€è‡´æ€§å­å›¾ (ç”¨äºç”Ÿæˆ)
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ”„ è‡ªä¸€è‡´æ€§æ£€ç´¢ (é‡‡æ ·{num_samples}æ¬¡)")
        print(f"{'=' * 60}\n")

        # 1. å¤šæ¬¡æ£€ç´¢
        all_subgraphs = []
        for i in range(num_samples):
            print(f"ç¬¬ {i + 1}/{num_samples} æ¬¡æ£€ç´¢...")
            subgraph = self.retrieve_relevant_subgraph(query, max_depth=2, top_k=8)
            all_subgraphs.append(subgraph)

        print(f"\nâœ“ å®Œæˆ {num_samples} æ¬¡æ£€ç´¢\n")

        # 2. ç»Ÿè®¡ä¸€è‡´æ€§
        node_counter = {}  # {(type, name): count}
        node_data = {}  # {(type, name): node_object}
        path_counter = {}  # {path_pattern: (count, path_object)}
        rel_counter = {}  # {(from, to, type): (count, rel_object)}

        for subgraph in all_subgraphs:
            # ç»Ÿè®¡èŠ‚ç‚¹
            for node in subgraph['nodes']:
                key = (node['type'], node['name'])
                node_counter[key] = node_counter.get(key, 0) + 1
                if key not in node_data:
                    node_data[key] = node

            # ç»Ÿè®¡å…³ç³»
            for rel in subgraph['relationships']:
                key = (rel['from_name'], rel['to_name'], rel['type'])
                rel_counter[key] = rel_counter.get(key, 0) + 1
                if key not in path_counter:
                    rel_counter[key] = (rel_counter[key], rel)

            # ç»Ÿè®¡è·¯å¾„æ¨¡å¼
            for path in subgraph['paths']:
                pattern = ' -> '.join([
                                          f"{path['nodes'][i]['name']}[{path['relationships'][i]['type']}]"
                                          for i in range(len(path['relationships']))
                                      ] + [path['nodes'][-1]['name']])

                path_counter[pattern] = path_counter.get(pattern, 0) + 1

        # 3. æ„å»ºé«˜ä¸€è‡´æ€§å­å›¾ (å…³é”®æ”¹è¿›)
        threshold = num_samples // 2 + 1  # è¶…è¿‡åŠæ•°

        # ç­›é€‰é«˜ä¸€è‡´æ€§èŠ‚ç‚¹
        consistent_nodes = []
        for (node_type, node_name), count in node_counter.items():
            if count >= threshold:
                node = node_data[(node_type, node_name)].copy()
                node['consistency'] = count / num_samples
                consistent_nodes.append(node)

        # ç­›é€‰é«˜ä¸€è‡´æ€§å…³ç³»
        consistent_relationships = []
        for (from_name, to_name, rel_type), count in rel_counter.items():
            if count >= threshold:
                # æ‰¾åˆ°å¯¹åº”çš„å…³ç³»å¯¹è±¡
                for subgraph in all_subgraphs:
                    for rel in subgraph['relationships']:
                        if (rel['from_name'] == from_name and
                                rel['to_name'] == to_name and
                                rel['type'] == rel_type):
                            rel_copy = rel.copy()
                            rel_copy['consistency'] = count / num_samples
                            consistent_relationships.append(rel_copy)
                            break
                    else:
                        continue
                    break

        # ç­›é€‰é«˜ä¸€è‡´æ€§è·¯å¾„
        consistent_paths = []
        for pattern, count in path_counter.items():
            if count >= threshold:
                # ä»åŸå§‹å­å›¾ä¸­æ‰¾åˆ°è¯¥è·¯å¾„
                for subgraph in all_subgraphs:
                    for path in subgraph['paths']:
                        path_pattern = ' -> '.join([
                                                       f"{path['nodes'][i]['name']}[{path['relationships'][i]['type']}]"
                                                       for i in range(len(path['relationships']))
                                                   ] + [path['nodes'][-1]['name']])

                        if path_pattern == pattern:
                            path_copy = path.copy()
                            path_copy['consistency'] = count / num_samples
                            consistent_paths.append(path_copy)
                            break
                    else:
                        continue
                    break

        print(f"âœ“ é«˜ä¸€è‡´æ€§å­å›¾æ„å»ºå®Œæˆ:")
        print(f"  - ä¸€è‡´æ€§èŠ‚ç‚¹: {len(consistent_nodes)} ä¸ª")
        print(f"  - ä¸€è‡´æ€§å…³ç³»: {len(consistent_relationships)} ä¸ª")
        print(f"  - ä¸€è‡´æ€§è·¯å¾„: {len(consistent_paths)} ä¸ª\n")

        # 4. è¿”å›é«˜ä¸€è‡´æ€§å­å›¾ (ç”¨äºç”Ÿæˆ)
        consistent_subgraph = {
            'nodes': consistent_nodes,
            'relationships': consistent_relationships,
            'paths': consistent_paths
        }

        return {
            'query': query,
            'num_samples': num_samples,
            'consistent_subgraph': consistent_subgraph,  # å…³é”®: è¿”å›ä¸€è‡´æ€§å­å›¾
            'all_subgraphs': all_subgraphs,
            'statistics': {
                'node_counter': node_counter,
                'path_counter': path_counter
            }
        }

    # ========== 3. åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ (æ”¹è¿›ç‰ˆ) ==========

    def controlled_generation_with_subgraph(self, query: str,
                                            use_consistency: bool = True,
                                            use_reasoning: bool = True) -> Dict[str, Any]:
        """
        åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ (æ”¹è¿›ç‰ˆ)
        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_consistency: æ˜¯å¦ä½¿ç”¨è‡ªä¸€è‡´æ€§æ£€ç´¢
            use_reasoning: æ˜¯å¦ä½¿ç”¨å¤šè·³æ¨ç†
        Returns:
            ç”Ÿæˆç»“æœ (åŒ…å«ç­”æ¡ˆå’ŒéªŒè¯)
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ¯ åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ (æ”¹è¿›ç‰ˆ)")
        print(f"{'=' * 60}\n")

        # 1. æ£€ç´¢é«˜ä¸€è‡´æ€§å­å›¾
        if use_consistency:
            consistency_result = self.self_consistency_retrieval(query, num_samples=3)
            # âœ“ ä½¿ç”¨é«˜ä¸€è‡´æ€§å­å›¾,ä¸æ˜¯éšæœºçš„ä¸€ä¸ª
            subgraph = consistency_result['consistent_subgraph']
            consistency_info = f"""
ä¸€è‡´æ€§åˆ†æ:
- é«˜ä¸€è‡´æ€§èŠ‚ç‚¹: {len(subgraph['nodes'])} ä¸ª
- é«˜ä¸€è‡´æ€§è·¯å¾„: {len(subgraph['paths'])} ä¸ª
- å¹³å‡ä¸€è‡´æ€§: {sum(n.get('consistency', 0) for n in subgraph['nodes']) / len(subgraph['nodes']):.2%}
"""
        else:
            subgraph = self.retrieve_relevant_subgraph(query, max_depth=2, top_k=10)
            consistency_info = ""

        # 2. å¤šè·³æ¨ç† (å…³é”®æ”¹è¿›)
        reasoning_chains = []
        if use_reasoning:
            print(" æ‰§è¡Œå¤šè·³æ¨ç†...\n")
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“
            entities = self._extract_entities_from_query(query)

            # ä¸ºæ¯å¯¹å®ä½“æ‰¾æ¨ç†è·¯å¾„
            if len(entities) >= 2:
                for i in range(len(entities) - 1):
                    reasoning = self.multi_hop_reasoning(
                        query=f"{entities[i]} å’Œ {entities[i + 1]} çš„å…³ç³»",
                        max_hops=3
                    )
                    if reasoning and reasoning.get('paths'):
                        reasoning_chains.append({
                            'from': entities[i],
                            'to': entities[i + 1],
                            'path': reasoning['paths'][0]  # æœ€ä½³è·¯å¾„
                        })

            print(f"âœ“ æ‰¾åˆ° {len(reasoning_chains)} æ¡æ¨ç†é“¾\n")

        # 3. æ„å»ºç»“æ„åŒ–çŸ¥è¯† (èåˆæ¨ç†é“¾)
        structured_knowledge = self._format_subgraph_with_reasoning(
            subgraph, reasoning_chains
        )

        # 4. ç¡¬çº¦æŸç”Ÿæˆ (å…³é”®æ”¹è¿›)
        print("ğŸ“ ç”Ÿæˆç­”æ¡ˆ (ç¡¬çº¦æŸæ¨¡å¼)...\n")
        answer, constrained_entities = self._generate_with_hard_constraints(
            query, structured_knowledge, consistency_info, subgraph
        )

        # 5. éªŒè¯
        validation = self.validate_generation_with_subgraph(answer, subgraph)

        return {
            'query': query,
            'answer': answer,
            'subgraph': subgraph,
            'reasoning_chains': reasoning_chains,
            'validation': validation,
            'constrained_entities': constrained_entities,
            'consistency_info': consistency_info
        }

    def _format_subgraph_with_reasoning(self, subgraph: Dict,
                                        reasoning_chains: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–å­å›¾ä¿¡æ¯,èåˆæ¨ç†é“¾
        """
        knowledge_parts = []

        # 1. æ ¼å¼åŒ–èŠ‚ç‚¹ä¿¡æ¯
        knowledge_parts.append("ã€ç›¸å…³åŒ»ç–—å®ä½“ã€‘")

        nodes_by_type = {}
        for node in subgraph['nodes']:
            node_type = node['type']
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node)

        for node_type, nodes in nodes_by_type.items():
            knowledge_parts.append(f"\n{node_type}:")
            for node in nodes[:8]:
                props = node.get('properties', {})
                consistency = node.get('consistency', 0)

                # è¿‡æ»¤æœ‰æ•ˆå±æ€§
                valid_props = {k: v for k, v in props.items()
                               if k not in ['id', 'name'] and v}

                if valid_props:
                    prop_str = ', '.join([f"{k}:{v}" for k, v in valid_props.items()])
                    if consistency > 0:
                        knowledge_parts.append(
                            f"  - {node['name']} ({prop_str}) [ä¸€è‡´æ€§:{consistency:.0%}]"
                        )
                    else:
                        knowledge_parts.append(f"  - {node['name']} ({prop_str})")
                else:
                    if consistency > 0:
                        knowledge_parts.append(
                            f"  - {node['name']} [ä¸€è‡´æ€§:{consistency:.0%}]"
                        )
                    else:
                        knowledge_parts.append(f"  - {node['name']}")

        # 2. æ ¼å¼åŒ–å…³ç³»è·¯å¾„
        knowledge_parts.append("\nã€åŒ»ç–—çŸ¥è¯†å…³è”ã€‘")

        # ä¼˜å…ˆæ˜¾ç¤ºé«˜ä¸€è‡´æ€§è·¯å¾„
        sorted_paths = sorted(
            subgraph['paths'][:10],
            key=lambda p: p.get('consistency', 0),
            reverse=True
        )

        for path in sorted_paths:
            consistency = path.get('consistency', 0)
            if consistency > 0:
                knowledge_parts.append(
                    f"  {path['description']} [ä¸€è‡´æ€§:{consistency:.0%}]"
                )
            else:
                knowledge_parts.append(f"  {path['description']}")

        # 3. èåˆæ¨ç†é“¾ (å…³é”®æ”¹è¿›)
        if reasoning_chains:
            knowledge_parts.append("\nã€æ¨ç†é“¾ã€‘")
            for chain in reasoning_chains:
                path = chain['path']
                knowledge_parts.append(f"\nä» {chain['from']} åˆ° {chain['to']} çš„æ¨ç†:")

                for i in range(len(path['relations'])):
                    knowledge_parts.append(
                        f"  æ­¥éª¤{i + 1}: {path['nodes'][i]} "
                        f"--[{path['relations'][i]}]--> {path['nodes'][i + 1]}"
                    )

        return '\n'.join(knowledge_parts)

    def _generate_with_hard_constraints(self, query: str, structured_knowledge: str,
                                        consistency_info: str, subgraph: Dict) -> Tuple[str, List[str]]:
        """
        ç¡¬çº¦æŸç”Ÿæˆ (å…³é”®æ”¹è¿›)

        ç­–ç•¥:
        1. æå–å­å›¾ä¸­çš„æ‰€æœ‰å®ä½“åç§°ä½œä¸º"å…è®¸åˆ—è¡¨"
        2. è¦æ±‚LLMåªä½¿ç”¨å…è®¸åˆ—è¡¨ä¸­çš„å®ä½“
        3. ç”ŸæˆåéªŒè¯å¹¶è¿‡æ»¤è¿è§„å†…å®¹
        """

        # æ„å»ºå®ä½“å…è®¸åˆ—è¡¨
        allowed_entities = [node['name'] for node in subgraph['nodes']]
        allowed_entities_str = ', '.join(allowed_entities)

        # æ„å»ºå…³ç³»å…è®¸åˆ—è¡¨
        allowed_relations = list(set([
            f"{rel['from_name']} â†’ {rel['type']} â†’ {rel['to_name']}"
            for rel in subgraph['relationships']
        ]))
        allowed_relations_str = '\n  '.join(allowed_relations[:20])

        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ã€ç¡¬çº¦æŸè§„åˆ™ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘
1. âš ï¸ åªèƒ½ä½¿ç”¨ä»¥ä¸‹å®ä½“åˆ—è¡¨ä¸­çš„å†…å®¹:
   {allowed_entities_str}

2. âš ï¸ åªèƒ½ä½¿ç”¨ä»¥ä¸‹å·²éªŒè¯çš„å…³ç³»:
   {allowed_relations_str}

3. âš ï¸ å¦‚æœè¦æåˆ°æŸä¸ªå®ä½“,å¿…é¡»ä»å…è®¸åˆ—è¡¨ä¸­é€‰æ‹©
4. âš ï¸ å¦‚æœçŸ¥è¯†å›¾è°±ä¿¡æ¯ä¸è¶³,æ˜ç¡®è¯´æ˜"å›¾è°±ä¸­æš‚æ— ç›¸å…³ä¿¡æ¯"
5. âš ï¸ åŒ…å«å®ä½“çš„å±æ€§ä¿¡æ¯(å‰‚é‡ã€æ—¶æœºã€é¢‘ç‡ç­‰)

{consistency_info}

ã€çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‘
{structured_knowledge}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”æ ¼å¼è¦æ±‚ã€‘
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç»„ç»‡ç­”æ¡ˆ:

1. ã€æ ¸å¿ƒç­”æ¡ˆã€‘(1-2å¥è¯æ€»ç»“)
2. ã€è¯¦ç»†è¯´æ˜ã€‘(åˆ†ç‚¹å±•å¼€,å¼•ç”¨å…·ä½“å®ä½“å’Œå…³ç³»)
3. ã€é‡è¦æç¤ºã€‘(å¦‚æœ‰ç‰¹æ®Šæ³¨æ„äº‹é¡¹)

âš ï¸ è®°ä½: æ¯ä¸ªå®ä½“åç§°å¿…é¡»å®Œå…¨æ¥è‡ªå…è®¸åˆ—è¡¨!

è¯·å›ç­”:
"""

        # ç”Ÿæˆç­”æ¡ˆ
        response = self._call_deepseek(prompt, max_tokens=800, temperature=0.1)

        # åå¤„ç†: éªŒè¯å’Œè¿‡æ»¤ (é¢å¤–çš„ç¡¬çº¦æŸå±‚)
        constrained_response, used_entities = self._enforce_entity_constraints(
            response, allowed_entities
        )

        return constrained_response, used_entities

    def _enforce_entity_constraints(self, text: str, allowed_entities: List[str]) -> Tuple[str, List[str]]:
        """
        å¼ºåˆ¶å®ä½“çº¦æŸ (åå¤„ç†ç¡¬çº¦æŸ)

        æ‰«ææ–‡æœ¬,æ ‡è®°ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­çš„å®ä½“
        """
        used_entities = []

        # æ‰¾å‡ºæ–‡æœ¬ä¸­ä½¿ç”¨çš„å®ä½“
        for entity in allowed_entities:
            if entity in text:
                used_entities.append(entity)

        # æ£€æµ‹å¯èƒ½çš„è¿è§„å®ä½“ (ç®€åŒ–ç‰ˆ,å®é™…å¯ç”¨NER)
        # è¿™é‡Œç”¨å¯å‘å¼æ–¹æ³•: æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åŒ»ç–—æœ¯è¯­
        suspicious_patterns = [
            r'(?<![a-zA-Z\u4e00-\u9fa5])[A-Z\u4e00-\u9fa5]{2,8}(?![a-zA-Z\u4e00-\u9fa5])',
        ]

        # åœ¨å®é™…åº”ç”¨ä¸­,å¯ä»¥ç”¨æ›´å¤æ‚çš„NERæ¨¡å‹æ£€æµ‹è¿è§„å®ä½“
        # è¿™é‡Œç®€åŒ–å¤„ç†,ä¸»è¦ä¾èµ–Promptçº¦æŸ

        return text, used_entities

    def _format_subgraph_for_generation(self, subgraph: Dict) -> str:
        """å°†å­å›¾æ ¼å¼åŒ–ä¸ºç»“æ„åŒ–çŸ¥è¯†"""
        knowledge_parts = []

        # æ ¼å¼åŒ–èŠ‚ç‚¹ä¿¡æ¯
        knowledge_parts.append("ã€ç›¸å…³åŒ»ç–—å®ä½“ã€‘")

        # æŒ‰ç±»å‹åˆ†ç»„
        nodes_by_type = {}
        for node in subgraph['nodes']:
            node_type = node['type']
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node)

        for node_type, nodes in nodes_by_type.items():
            knowledge_parts.append(f"\n{node_type}:")
            for node in nodes[:5]:  # é™åˆ¶æ•°é‡
                props = node.get('properties', {})
                prop_str = ', '.join([f"{k}:{v}" for k, v in props.items()
                                      if k not in ['id', 'name']])
                if prop_str:
                    knowledge_parts.append(f"  - {node['name']} ({prop_str})")
                else:
                    knowledge_parts.append(f"  - {node['name']}")

        # æ ¼å¼åŒ–å…³ç³»å’Œè·¯å¾„
        knowledge_parts.append("\nã€åŒ»ç–—çŸ¥è¯†å…³è”ã€‘")
        for path in subgraph['paths'][:5]:  # é™åˆ¶è·¯å¾„æ•°é‡
            knowledge_parts.append(f"  {path['description']}")

        return '\n'.join(knowledge_parts)

    def _generate_with_constraints(self, query: str, structured_knowledge: str,
                                   consistency_info: str) -> str:
        """åŸºäºçº¦æŸç”Ÿæˆç­”æ¡ˆ"""
        print("ğŸ“ ç”Ÿæˆç­”æ¡ˆ...\n")

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ã€é‡è¦çº¦æŸã€‘
1. å¿…é¡»åŸºäºæä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”
2. ä¸è¦ç¼–é€ çŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰çš„ä¿¡æ¯
3. å¦‚æœçŸ¥è¯†å›¾è°±ä¿¡æ¯ä¸è¶³,æ˜ç¡®è¯´æ˜
4. æŒ‰ç…§"ç–¾ç—…è¯†åˆ« -> æ²»ç–—æªæ–½ -> ç”¨è¯æŒ‡å¯¼ -> ç›‘æµ‹è¦ç‚¹"çš„ç»“æ„ç»„ç»‡ç­”æ¡ˆ
5. å¼•ç”¨å…·ä½“çš„å®ä½“å’Œå…³ç³»

{consistency_info}

ã€çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‘
{structured_knowledge}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- ç»“æ„æ¸…æ™°,åˆ†ç‚¹ä½œç­”
- å¼•ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„å…·ä½“ä¿¡æ¯
- æ ‡æ³¨ä¿¡æ¯æ¥æº(å¦‚"æ ¹æ®çŸ¥è¯†å›¾è°±...")
- å¦‚æœ‰å±æ€§ä¿¡æ¯(å‰‚é‡ã€æ—¶æœºç­‰),åŠ¡å¿…åŒ…å«

è¯·å›ç­”:
"""

        response = self._call_deepseek(prompt, max_tokens=1000, temperature=0.3)

        return response

    # ========== 4. å¤šè·³æ¨ç† ==========

    def multi_hop_reasoning(self, query: str, max_hops: int = 3) -> Dict[str, Any]:
        """
        å¤šè·³æ¨ç†
        Args:
            query: æŸ¥è¯¢
            max_hops: æœ€å¤§æ¨ç†è·³æ•°
        Returns:
            æ¨ç†é“¾
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ§  å¤šè·³æ¨ç† (æœ€å¤§{max_hops}è·³)")
        print(f"{'=' * 60}\n")

        # 1. æå–èµ·å§‹å®ä½“å’Œç›®æ ‡
        entities = self._extract_entities_from_query(query)
        if len(entities) < 2:
            print("âœ— éœ€è¦è‡³å°‘2ä¸ªå®ä½“è¿›è¡Œå¤šè·³æ¨ç†\n")
            return None

        start_entity = entities[0]
        end_entity = entities[-1]

        print(f"èµ·å§‹å®ä½“: {start_entity}")
        print(f"ç›®æ ‡å®ä½“: {end_entity}\n")

        # 2. æŸ¥æ‰¾æ¨ç†è·¯å¾„
        reasoning_paths = self._find_reasoning_paths(start_entity, end_entity, max_hops)

        print(f"âœ“ æ‰¾åˆ° {len(reasoning_paths)} æ¡æ¨ç†è·¯å¾„\n")

        # 3. è¯„åˆ†å’Œæ’åº
        scored_paths = self._score_reasoning_paths(reasoning_paths)

        return {
            'start': start_entity,
            'end': end_entity,
            'paths': scored_paths
        }

    def _find_reasoning_paths(self, start: str, end: str, max_hops: int) -> List[Dict]:
        """æŸ¥æ‰¾æ¨ç†è·¯å¾„"""
        with self.driver.session() as session:
            result = session.run(f"""
                MATCH path = (start)-[*1..{max_hops}]-(end)
                WHERE start.name CONTAINS $start AND end.name CONTAINS $end
                WITH path,
                     [node in nodes(path) | node.name] as node_names,
                     [rel in relationships(path) | type(rel)] as rel_types,
                     length(path) as hops
                RETURN node_names, rel_types, hops
                ORDER BY hops ASC
                LIMIT 10
            """, start=start, end=end)

            paths = []
            for record in result:
                paths.append({
                    'nodes': record['node_names'],
                    'relations': record['rel_types'],
                    'hops': record['hops']
                })

            return paths

    def _score_reasoning_paths(self, paths: List[Dict]) -> List[Dict]:
        """è¯„åˆ†æ¨ç†è·¯å¾„"""
        for path in paths:
            # è¯„åˆ†å› ç´ :
            # 1. è·¯å¾„é•¿åº¦(è¶ŠçŸ­è¶Šå¥½)
            length_score = 1.0 / (path['hops'] + 1)

            # 2. å…³ç³»ç±»å‹é‡è¦æ€§
            important_rels = ['éœ€è¦æ²»ç–—', 'ä½¿ç”¨è¯ç‰©', 'éœ€è¦æ£€æŸ¥']
            rel_score = sum(1 for r in path['relations'] if r in important_rels) / len(path['relations'])

            # ç»¼åˆè¯„åˆ†
            path['score'] = 0.6 * length_score + 0.4 * rel_score

        # æ’åº
        paths.sort(key=lambda x: x['score'], reverse=True)

        return paths

    # ========== 5. å­å›¾éªŒè¯ ==========

    def validate_generation_with_subgraph(self, generated_answer: str,
                                          subgraph: Dict) -> Dict[str, Any]:
        """
        éªŒè¯ç”Ÿæˆå†…å®¹æ˜¯å¦ä¸å­å›¾ä¸€è‡´
        Args:
            generated_answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            subgraph: å‚è€ƒå­å›¾
        Returns:
            éªŒè¯ç»“æœ
        """
        print(f"\n{'=' * 60}")
        print(f"âœ… éªŒè¯ç”Ÿæˆå†…å®¹")
        print(f"{'=' * 60}\n")

        # 1. æå–ç­”æ¡ˆä¸­çš„å®ä½“
        answer_entities = self._extract_entities_from_text(generated_answer)

        # 2. æ£€æŸ¥å®ä½“æ˜¯å¦åœ¨å­å›¾ä¸­
        subgraph_entity_names = {node['name'] for node in subgraph['nodes']}

        valid_entities = [e for e in answer_entities if e in subgraph_entity_names]
        invalid_entities = [e for e in answer_entities if e not in subgraph_entity_names]

        # 3. æ£€æŸ¥å…³ç³»é™ˆè¿°
        relation_claims = self._extract_relation_claims(generated_answer)
        verified_claims = self._verify_claims_with_subgraph(relation_claims, subgraph)

        # 4. è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
        entity_consistency = len(valid_entities) / len(answer_entities) if answer_entities else 0
        claim_consistency = sum(1 for c in verified_claims if c['verified']) / len(
            verified_claims) if verified_claims else 0

        overall_score = 0.5 * entity_consistency + 0.5 * claim_consistency

        print(f"éªŒè¯ç»“æœ:")
        print(f"  - å®ä½“ä¸€è‡´æ€§: {entity_consistency:.2%}")
        print(f"  - å…³ç³»ä¸€è‡´æ€§: {claim_consistency:.2%}")
        print(f"  - æ€»ä½“ä¸€è‡´æ€§: {overall_score:.2%}\n")

        return {
            'overall_score': overall_score,
            'entity_consistency': entity_consistency,
            'claim_consistency': claim_consistency,
            'valid_entities': valid_entities,
            'invalid_entities': invalid_entities,
            'verified_claims': verified_claims
        }

    def _extract_entities_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        # ç®€åŒ–ç‰ˆæœ¬:åŒ¹é…å›¾è°±ä¸­çš„èŠ‚ç‚¹åç§°
        entities = []
        with self.driver.session() as session:
            result = session.run("""
                MATCH (n)
                WHERE $text CONTAINS n.name
                RETURN DISTINCT n.name as name
            """, text=text)
            entities = [record['name'] for record in result]

        return entities

    def _extract_relation_claims(self, text: str) -> List[str]:
        """æå–å…³ç³»é™ˆè¿°"""
        # ç®€åŒ–ç‰ˆæœ¬:æå–å¥å­
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        return sentences

    def _verify_claims_with_subgraph(self, claims: List[str],
                                     subgraph: Dict) -> List[Dict]:
        """éªŒè¯é™ˆè¿°ä¸å­å›¾çš„ä¸€è‡´æ€§"""
        verified = []

        for claim in claims:
            # æ£€æŸ¥é™ˆè¿°ä¸­æ˜¯å¦åŒ…å«å­å›¾çš„è·¯å¾„
            is_verified = False
            supporting_path = None

            for path in subgraph['paths']:
                # ç®€å•æ£€æŸ¥:å¦‚æœé™ˆè¿°åŒ…å«è·¯å¾„ä¸­çš„å…³é”®å®ä½“
                path_entities = [node['name'] for node in path['nodes']]
                if any(entity in claim for entity in path_entities[:2]):
                    is_verified = True
                    supporting_path = path['description']
                    break

            verified.append({
                'claim': claim,
                'verified': is_verified,
                'supporting_path': supporting_path
            })

        return verified

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _call_deepseek(self, prompt: str, max_tokens: int = 1000,
                       temperature: float = 0) -> str:
        """è°ƒç”¨DeepSeek API"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.deepseek_api_key}"
        }

        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": temperature,
            "max_tokens": max_tokens
        }

        response = requests.post(self.deepseek_api_url, headers=headers,
                                 json=payload, timeout=60)
        response.raise_for_status()

        response_data = response.json()
        return response_data['choices'][0]['message']['content']