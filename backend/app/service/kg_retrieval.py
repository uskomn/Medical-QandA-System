import json
import re
from typing import List, Dict, Any, Tuple
from neo4j import GraphDatabase
import requests
import os

class KnowledgeGraphRetrieval:
    """çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸æ¨ç†ç³»ç»Ÿ"""

    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str,
                 deepseek_api_key: str = None):
        """åˆå§‹åŒ–"""
        # éªŒè¯ Neo4j è¿æ¥
        try:
            self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))
            with self.driver.session() as session:
                session.run("RETURN 1")
            print("âœ“ Neo4j è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âœ— Neo4j è¿æ¥å¤±è´¥: {e}")
            raise

        # éªŒè¯ DeepSeek API
        self.deepseek_api_key = deepseek_api_key or os.getenv('DEEPSEEK_API_KEY')
        if not self.deepseek_api_key or self.deepseek_api_key == "your_api_key":
            print("âš ï¸  DeepSeek API key æœªé…ç½®,å°†ä½¿ç”¨ç®€åŒ–æ–¹æ³•")
            self.use_llm = False
        else:
            self.use_llm = True
            print("âœ“ DeepSeek API key å·²é…ç½®")

        self.deepseek_api_url = "https://api.deepseek.com/v1/chat/completions"

    def close(self):
        """å…³é—­è¿æ¥"""
        self.driver.close()

    # ========== 1. å›¾æ£€ç´¢æ¨¡å— ==========

    def retrieve_relevant_subgraph(self, query: str, max_depth: int = 2,
                                   top_k: int = 10) -> Dict[str, Any]:

        entities = self._extract_entities_from_query(query)
        matched_nodes = self._find_matching_nodes(entities)
        if not matched_nodes:
            print("âœ— æœªæ‰¾åˆ°åŒ¹é…èŠ‚ç‚¹\n")
            return {"nodes": [], "relationships": [], "paths": []}

        subgraph = self._expand_subgraph(matched_nodes, max_depth, top_k)
        print(f"âœ“ æ‰©å±•å­å›¾å®Œæˆ:")
        print(f"  - èŠ‚ç‚¹æ•°: {len(subgraph['nodes'])}")
        print(f"  - å…³ç³»æ•°: {len(subgraph['relationships'])}")
        print(f"  - è·¯å¾„æ•°: {len(subgraph['paths'])}\n")

        return subgraph

    def _extract_entities_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®å®ä½“"""
        if self.use_llm:
            prompt = f"""ä»ä»¥ä¸‹åŒ»ç–—é—®é¢˜ä¸­æå–å…³é”®å®ä½“(ç–¾ç—…ã€æ²»ç–—ã€è¯ç‰©ã€æ£€æŸ¥ç­‰)ã€‚

é—®é¢˜: {query}

åªè¿”å›JSONæ•°ç»„,æ ¼å¼: ["å®ä½“1", "å®ä½“2", ...]
"""
            try:
                response = self._call_deepseek(prompt, max_tokens=200, temperature=0)
                response = re.sub(r'```json\s*', '', response.strip())
                response = re.sub(r'```\s*', '', response)
                entities = json.loads(response)
                if isinstance(entities, list) and entities:
                    return entities
            except Exception as e:
                print(f"âš ï¸  LLM æå–å¤±è´¥: {e}, ä½¿ç”¨å¤‡ç”¨æ–¹æ³•")

        # å¤‡ç”¨æ–¹æ³•
        keywords = []
        try:
            with self.driver.session() as session:
                result = session.run("""
                    MATCH (n)
                    WHERE $query_text CONTAINS n.name
                    RETURN DISTINCT n.name as name
                    LIMIT 10
                """, query_text=query)
                keywords = [record['name'] for record in result]
        except Exception as e:
            print(f"âš ï¸  å›¾è°±åŒ¹é…å¤±è´¥: {e}")

        return keywords if keywords else [query]

    def _find_matching_nodes(self, entities: List[str]) -> List[Dict]:
        """æŸ¥æ‰¾åŒ¹é…çš„å›¾è°±èŠ‚ç‚¹"""
        matched = []
        with self.driver.session() as session:
            for entity in entities:
                result = session.run("""
                    MATCH (n)
                    WHERE n.name CONTAINS $entity
                    RETURN id(n) as node_id, 
                           labels(n)[0] as type,
                           n.name as name,
                           properties(n) as properties
                    LIMIT 5
                """, entity=entity)

                for record in result:
                    matched.append({
                        'id': record['node_id'],
                        'type': record['type'],
                        'name': record['name'],
                        'properties': dict(record['properties'])
                    })
        return matched

    def _expand_subgraph(self, seed_nodes: List[Dict], max_depth: int,
                         top_k: int) -> Dict[str, Any]:
        """æ‰©å±•å­å›¾"""
        node_ids = [node['id'] for node in seed_nodes]

        with self.driver.session() as session:
            result = session.run(f"""
                MATCH path = (start)-[*1..{max_depth}]-(end)
                WHERE id(start) IN $node_ids
                WITH path, 
                     length(path) as path_length,
                     [rel in relationships(path) | type(rel)] as rel_types
                RETURN path,
                       [node in nodes(path) | {{
                           id: id(node),
                           type: labels(node)[0],
                           name: node.name,
                           properties: properties(node)
                       }}] as nodes,
                       [rel in relationships(path) | {{
                           type: type(rel),
                           properties: properties(rel)
                       }}] as relationships,
                       path_length,
                       rel_types
                ORDER BY path_length ASC
                LIMIT $top_k
            """, node_ids=node_ids, top_k=top_k)

            all_nodes = {}
            all_relationships = []
            all_paths = []

            for record in result:
                nodes = record['nodes']
                rels = record['relationships']

                for node in nodes:
                    node_id = node['id']
                    if node_id not in all_nodes:
                        all_nodes[node_id] = node

                for i, rel in enumerate(rels):
                    rel_data = {
                        'from': nodes[i]['id'],
                        'from_name': nodes[i]['name'],
                        'to': nodes[i + 1]['id'],
                        'to_name': nodes[i + 1]['name'],
                        'type': rel['type'],
                        'properties': rel['properties']
                    }
                    all_relationships.append(rel_data)

                path_desc = ' -> '.join([
                                            f"{nodes[i]['name']}[{rels[i]['type']}]"
                                            for i in range(len(rels))
                                        ] + [nodes[-1]['name']])

                all_paths.append({
                    'nodes': nodes,
                    'relationships': rels,
                    'description': path_desc,
                    'length': record['path_length']
                })

            return {
                'nodes': list(all_nodes.values()),
                'relationships': all_relationships,
                'paths': all_paths
            }

    # ========== 2. åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ ==========

    def controlled_generation_with_subgraph(self, query: str,
                                            use_consistency: bool = True,
                                            use_reasoning: bool = True) -> Dict[str, Any]:
        """
        åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ
        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_consistency: æ˜¯å¦ä½¿ç”¨è‡ªä¸€è‡´æ€§æ£€ç´¢
            use_reasoning: æ˜¯å¦ä½¿ç”¨å¤šè·³æ¨ç†
        Returns:
            ç”Ÿæˆç»“æœ (åŒ…å«ç­”æ¡ˆå’ŒéªŒè¯)
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ¯ åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ")
        print(f"{'=' * 60}\n")

        # 1. æ£€ç´¢é«˜ä¸€è‡´æ€§å­å›¾
        if use_consistency:
            consistency_result = self.self_consistency_retrieval(query, num_samples=3)
            subgraph = consistency_result['consistent_subgraph']
            consistency_info = f"""
ä¸€è‡´æ€§åˆ†æ:
- é«˜ä¸€è‡´æ€§èŠ‚ç‚¹: {len(subgraph['nodes'])} ä¸ª
- é«˜ä¸€è‡´æ€§è·¯å¾„: {len(subgraph['paths'])} ä¸ª
- å¹³å‡ä¸€è‡´æ€§: {sum(n.get('consistency', 0) for n in subgraph['nodes']) / len(subgraph['nodes']) if subgraph['nodes'] else 0:.2%}
"""
        else:
            subgraph = self.retrieve_relevant_subgraph(query, max_depth=2, top_k=10)
            consistency_info = ""

        # 2. å¤šè·³æ¨ç† (å¦‚æœå¯ç”¨)
        reasoning_chains = []
        if use_reasoning:
            print("ğŸ§  æ‰§è¡Œå¤šè·³æ¨ç†...\n")
            entities = self._extract_entities_from_query(query)

            if len(entities) >= 2:
                for i in range(len(entities) - 1):
                    reasoning = self.multi_hop_reasoning(
                        query=f"{entities[i]} å’Œ {entities[i + 1]} çš„å…³ç³»",
                        max_hops=3
                    )
                    if reasoning and reasoning.get('paths'):
                        reasoning_chains.append({
                            'from': entities[i],
                            'to': entities[i + 1],
                            'path': reasoning['paths'][0]
                        })

            print(f"âœ“ æ‰¾åˆ° {len(reasoning_chains)} æ¡æ¨ç†é“¾\n")

        # 3. æ„å»ºç»“æ„åŒ–çŸ¥è¯†
        structured_knowledge = self._format_subgraph_with_reasoning(
            subgraph, reasoning_chains
        )

        # 4. ç¡¬çº¦æŸç”Ÿæˆ
        print("ğŸ“ ç”Ÿæˆç­”æ¡ˆ (ç¡¬çº¦æŸæ¨¡å¼)...\n")
        answer, constrained_entities = self._generate_with_hard_constraints(
            query, structured_knowledge, consistency_info, subgraph
        )

        # 5. éªŒè¯
        validation = self.validate_generation_with_subgraph(answer, subgraph)

        return {
            'query': query,
            'answer': answer,
            'subgraph': subgraph,
            'reasoning_chains': reasoning_chains,
            'validation': validation,
            'constrained_entities': constrained_entities,
            'consistency_info': consistency_info
        }

    def _format_subgraph_with_reasoning(self, subgraph: Dict,
                                        reasoning_chains: List[Dict]) -> str:
        """æ ¼å¼åŒ–å­å›¾ä¿¡æ¯,èåˆæ¨ç†é“¾"""
        knowledge_parts = []

        # 1. æ ¼å¼åŒ–èŠ‚ç‚¹ä¿¡æ¯
        knowledge_parts.append("ã€ç›¸å…³åŒ»ç–—å®ä½“ã€‘")

        nodes_by_type = {}
        for node in subgraph['nodes']:
            node_type = node['type']
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node)

        for node_type, nodes in nodes_by_type.items():
            knowledge_parts.append(f"\n{node_type}:")
            for node in nodes[:8]:
                props = node.get('properties', {})
                consistency = node.get('consistency', 0)

                # è¿‡æ»¤æœ‰æ•ˆå±æ€§
                valid_props = {k: v for k, v in props.items()
                               if k not in ['id', 'name'] and v}

                if valid_props:
                    prop_str = ', '.join([f"{k}:{v}" for k, v in valid_props.items()])
                    if consistency > 0:
                        knowledge_parts.append(
                            f"  - {node['name']} ({prop_str}) [ä¸€è‡´æ€§:{consistency:.0%}]"
                        )
                    else:
                        knowledge_parts.append(f"  - {node['name']} ({prop_str})")
                else:
                    if consistency > 0:
                        knowledge_parts.append(
                            f"  - {node['name']} [ä¸€è‡´æ€§:{consistency:.0%}]"
                        )
                    else:
                        knowledge_parts.append(f"  - {node['name']}")

        # 2. æ ¼å¼åŒ–å…³ç³»è·¯å¾„
        knowledge_parts.append("\nã€åŒ»ç–—çŸ¥è¯†å…³è”ã€‘")

        sorted_paths = sorted(
            subgraph['paths'][:10],
            key=lambda p: p.get('consistency', 0),
            reverse=True
        )

        for path in sorted_paths:
            consistency = path.get('consistency', 0)
            if consistency > 0:
                knowledge_parts.append(
                    f"  {path['description']} [ä¸€è‡´æ€§:{consistency:.0%}]"
                )
            else:
                knowledge_parts.append(f"  {path['description']}")

        # 3. èåˆæ¨ç†é“¾
        if reasoning_chains:
            knowledge_parts.append("\nã€æ¨ç†é“¾ã€‘")
            for chain in reasoning_chains:
                path = chain['path']
                knowledge_parts.append(f"\nä» {chain['from']} åˆ° {chain['to']} çš„æ¨ç†:")

                for i in range(len(path['relations'])):
                    knowledge_parts.append(
                        f"  æ­¥éª¤{i + 1}: {path['nodes'][i]} "
                        f"--[{path['relations'][i]}]--> {path['nodes'][i + 1]}"
                    )

        return '\n'.join(knowledge_parts)

    def _generate_with_hard_constraints(self, query: str, structured_knowledge: str,
                                        consistency_info: str, subgraph: Dict) -> Tuple[str, List[str]]:
        """ç¡¬çº¦æŸç”Ÿæˆ"""
        # æ„å»ºå®ä½“å…è®¸åˆ—è¡¨
        allowed_entities = [node['name'] for node in subgraph['nodes']]
        allowed_entities_str = ', '.join(allowed_entities)

        # æ„å»ºå…³ç³»å…è®¸åˆ—è¡¨
        allowed_relations = list(set([
            f"{rel['from_name']} â†’ {rel['type']} â†’ {rel['to_name']}"
            for rel in subgraph['relationships']
        ]))
        allowed_relations_str = '\n  '.join(allowed_relations[:20])

        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ã€ç¡¬çº¦æŸè§„åˆ™ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘
1. âš ï¸ åªèƒ½ä½¿ç”¨ä»¥ä¸‹å®ä½“åˆ—è¡¨ä¸­çš„å†…å®¹:
   {allowed_entities_str}

2. âš ï¸ åªèƒ½ä½¿ç”¨ä»¥ä¸‹å·²éªŒè¯çš„å…³ç³»:
   {allowed_relations_str}

3. âš ï¸ å¦‚æœè¦æåˆ°æŸä¸ªå®ä½“,å¿…é¡»ä»å…è®¸åˆ—è¡¨ä¸­é€‰æ‹©
4. âš ï¸ å¦‚æœçŸ¥è¯†å›¾è°±ä¿¡æ¯ä¸è¶³,æ˜ç¡®è¯´æ˜"å›¾è°±ä¸­æš‚æ— ç›¸å…³ä¿¡æ¯"
5. âš ï¸ åŒ…å«å®ä½“çš„å±æ€§ä¿¡æ¯(å‰‚é‡ã€æ—¶æœºã€é¢‘ç‡ç­‰)

{consistency_info}

ã€çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‘
{structured_knowledge}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”æ ¼å¼è¦æ±‚ã€‘
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç»„ç»‡ç­”æ¡ˆ:

1. ã€æ ¸å¿ƒç­”æ¡ˆã€‘(1-2å¥è¯æ€»ç»“)
2. ã€è¯¦ç»†è¯´æ˜ã€‘(åˆ†ç‚¹å±•å¼€,å¼•ç”¨å…·ä½“å®ä½“å’Œå…³ç³»)
3. ã€é‡è¦æç¤ºã€‘(å¦‚æœ‰ç‰¹æ®Šæ³¨æ„äº‹é¡¹)

âš ï¸ è®°ä½: æ¯ä¸ªå®ä½“åç§°å¿…é¡»å®Œå…¨æ¥è‡ªå…è®¸åˆ—è¡¨!

è¯·å›ç­”:
"""

        # ç”Ÿæˆç­”æ¡ˆ
        if self.use_llm:
            try:
                response = self._call_deepseek(prompt, max_tokens=800, temperature=0.1)
            except Exception as e:
                print(f"âš ï¸  LLM ç”Ÿæˆå¤±è´¥: {e}")
                response = self._generate_fallback_answer(query, subgraph)
        else:
            response = self._generate_fallback_answer(query, subgraph)

        # åå¤„ç†: éªŒè¯å’Œè¿‡æ»¤
        constrained_response, used_entities = self._enforce_entity_constraints(
            response, allowed_entities
        )

        return constrained_response, used_entities

    def _generate_fallback_answer(self, query: str, subgraph: Dict) -> str:
        """å¤‡ç”¨ç”Ÿæˆæ–¹æ³• (ä¸ä½¿ç”¨LLM)"""
        answer_parts = []

        answer_parts.append("ã€åŸºäºçŸ¥è¯†å›¾è°±çš„å›ç­”ã€‘\n")

        if subgraph['nodes']:
            answer_parts.append("ç›¸å…³å®ä½“:")
            for node in subgraph['nodes'][:5]:
                answer_parts.append(f"  - {node['name']} ({node['type']})")

        if subgraph['paths']:
            answer_parts.append("\nç›¸å…³çŸ¥è¯†:")
            for path in subgraph['paths'][:3]:
                answer_parts.append(f"  - {path['description']}")

        return '\n'.join(answer_parts)

    def _enforce_entity_constraints(self, text: str, allowed_entities: List[str]) -> Tuple[str, List[str]]:
        """å¼ºåˆ¶å®ä½“çº¦æŸ (åå¤„ç†ç¡¬çº¦æŸ)"""
        used_entities = []

        # æ‰¾å‡ºæ–‡æœ¬ä¸­ä½¿ç”¨çš„å®ä½“
        for entity in allowed_entities:
            if entity in text:
                used_entities.append(entity)

        return text, used_entities

    # ========== 3. è‡ªä¸€è‡´æ€§æ£€ç´¢ ==========

    def self_consistency_retrieval(self, query: str, num_samples: int = 3) -> Dict[str, Any]:
        """è‡ªä¸€è‡´æ€§æ£€ç´¢"""
        print(f"\n{'=' * 60}")
        print(f"ğŸ”„ è‡ªä¸€è‡´æ€§æ£€ç´¢ (é‡‡æ ·{num_samples}æ¬¡)")
        print(f"{'=' * 60}\n")

        all_subgraphs = []
        for i in range(num_samples):
            print(f"ç¬¬ {i + 1}/{num_samples} æ¬¡æ£€ç´¢...")
            subgraph = self.retrieve_relevant_subgraph(query, max_depth=2, top_k=8)
            all_subgraphs.append(subgraph)

        print(f"\nâœ“ å®Œæˆ {num_samples} æ¬¡æ£€ç´¢\n")

        # ç»Ÿè®¡ä¸€è‡´æ€§
        node_counter = {}
        node_data = {}
        path_counter = {}
        rel_counter = {}
        rel_data = {}

        for subgraph in all_subgraphs:
            for node in subgraph['nodes']:
                key = (node['type'], node['name'])
                node_counter[key] = node_counter.get(key, 0) + 1
                if key not in node_data:
                    node_data[key] = node

            for rel in subgraph['relationships']:
                key = (rel['from_name'], rel['to_name'], rel['type'])
                rel_counter[key] = rel_counter.get(key, 0) + 1
                if key not in rel_data:
                    rel_data[key] = rel

            for path in subgraph['paths']:
                pattern = ' -> '.join([
                                          f"{path['nodes'][i]['name']}[{path['relationships'][i]['type']}]"
                                          for i in range(len(path['relationships']))
                                      ] + [path['nodes'][-1]['name']])
                path_counter[pattern] = path_counter.get(pattern, 0) + 1

        # æ„å»ºé«˜ä¸€è‡´æ€§å­å›¾
        threshold = num_samples // 2 + 1

        consistent_nodes = []
        for (node_type, node_name), count in node_counter.items():
            if count >= threshold:
                node = node_data[(node_type, node_name)].copy()
                node['consistency'] = count / num_samples
                consistent_nodes.append(node)

        consistent_relationships = []
        for (from_name, to_name, rel_type), count in rel_counter.items():
            if count >= threshold:
                rel = rel_data[(from_name, to_name, rel_type)].copy()
                rel['consistency'] = count / num_samples
                consistent_relationships.append(rel)

        consistent_paths = []
        path_data = {}
        for subgraph in all_subgraphs:
            for path in subgraph['paths']:
                pattern = ' -> '.join([
                                          f"{path['nodes'][i]['name']}[{path['relationships'][i]['type']}]"
                                          for i in range(len(path['relationships']))
                                      ] + [path['nodes'][-1]['name']])
                if pattern not in path_data:
                    path_data[pattern] = path

        for pattern, count in path_counter.items():
            if count >= threshold and pattern in path_data:
                path = path_data[pattern].copy()
                path['consistency'] = count / num_samples
                consistent_paths.append(path)

        print(f"âœ“ é«˜ä¸€è‡´æ€§å­å›¾æ„å»ºå®Œæˆ:")
        print(f"  - ä¸€è‡´æ€§èŠ‚ç‚¹: {len(consistent_nodes)} ä¸ª")
        print(f"  - ä¸€è‡´æ€§å…³ç³»: {len(consistent_relationships)} ä¸ª")
        print(f"  - ä¸€è‡´æ€§è·¯å¾„: {len(consistent_paths)} ä¸ª\n")

        consistent_subgraph = {
            'nodes': consistent_nodes,
            'relationships': consistent_relationships,
            'paths': consistent_paths
        }

        return {
            'query': query,
            'num_samples': num_samples,
            'consistent_subgraph': consistent_subgraph,
            'all_subgraphs': all_subgraphs,
            'statistics': {
                'node_counter': node_counter,
                'path_counter': path_counter
            }
        }

    # ========== 4. å¤šè·³æ¨ç† ==========

    def multi_hop_reasoning(self, query: str, max_hops: int = 3) -> Dict[str, Any]:
        """
        å¤šè·³æ¨ç†
        Args:
            query: æŸ¥è¯¢
            max_hops: æœ€å¤§æ¨ç†è·³æ•°
        Returns:
            æ¨ç†é“¾
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ§  å¤šè·³æ¨ç† (æœ€å¤§{max_hops}è·³)")
        print(f"{'=' * 60}\n")

        # 1. æå–èµ·å§‹å®ä½“å’Œç›®æ ‡
        entities = self._extract_entities_from_query(query)
        if len(entities) < 2:
            print("âœ— éœ€è¦è‡³å°‘2ä¸ªå®ä½“è¿›è¡Œå¤šè·³æ¨ç†\n")
            return None

        start_entity = entities[0]
        end_entity = entities[-1]

        print(f"èµ·å§‹å®ä½“: {start_entity}")
        print(f"ç›®æ ‡å®ä½“: {end_entity}\n")

        # 2. æŸ¥æ‰¾æ¨ç†è·¯å¾„
        reasoning_paths = self._find_reasoning_paths(start_entity, end_entity, max_hops)

        print(f"âœ“ æ‰¾åˆ° {len(reasoning_paths)} æ¡æ¨ç†è·¯å¾„\n")

        # 3. è¯„åˆ†å’Œæ’åº
        scored_paths = self._score_reasoning_paths(reasoning_paths)

        return {
            'start': start_entity,
            'end': end_entity,
            'paths': scored_paths
        }

    def _find_reasoning_paths(self, start: str, end: str, max_hops: int) -> List[Dict]:
        """æŸ¥æ‰¾æ¨ç†è·¯å¾„"""
        with self.driver.session() as session:
            result = session.run(f"""
                MATCH path = (start)-[*1..{max_hops}]-(end)
                WHERE start.name CONTAINS $start AND end.name CONTAINS $end
                WITH path,
                     [node in nodes(path) | node.name] as node_names,
                     [rel in relationships(path) | type(rel)] as rel_types,
                     length(path) as hops
                RETURN node_names, rel_types, hops
                ORDER BY hops ASC
                LIMIT 10
            """, start=start, end=end)

            paths = []
            for record in result:
                paths.append({
                    'nodes': record['node_names'],
                    'relations': record['rel_types'],
                    'hops': record['hops']
                })

            return paths

    def _score_reasoning_paths(self, paths: List[Dict]) -> List[Dict]:
        """è¯„åˆ†æ¨ç†è·¯å¾„"""
        for path in paths:
            # è¯„åˆ†å› ç´ :
            # 1. è·¯å¾„é•¿åº¦(è¶ŠçŸ­è¶Šå¥½)
            length_score = 1.0 / (path['hops'] + 1)

            # 2. å…³ç³»ç±»å‹é‡è¦æ€§
            important_rels = ['éœ€è¦æ²»ç–—', 'ä½¿ç”¨è¯ç‰©', 'éœ€è¦æ£€æŸ¥']
            rel_score = sum(1 for r in path['relations'] if r in important_rels) / len(path['relations']) if path[
                'relations'] else 0

            # ç»¼åˆè¯„åˆ†
            path['score'] = 0.6 * length_score + 0.4 * rel_score

        # æ’åº
        paths.sort(key=lambda x: x['score'], reverse=True)

        return paths

    # ========== 5. å­å›¾éªŒè¯ ==========

    def validate_generation_with_subgraph(self, generated_answer: str,
                                          subgraph: Dict) -> Dict[str, Any]:
        """
        éªŒè¯ç”Ÿæˆå†…å®¹æ˜¯å¦ä¸å­å›¾ä¸€è‡´
        Args:
            generated_answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            subgraph: å‚è€ƒå­å›¾
        Returns:
            éªŒè¯ç»“æœ
        """
        print(f"\n{'=' * 60}")
        print(f"âœ… éªŒè¯ç”Ÿæˆå†…å®¹")
        print(f"{'=' * 60}\n")

        # 1. æå–ç­”æ¡ˆä¸­çš„å®ä½“
        answer_entities = self._extract_entities_from_text(generated_answer)

        # 2. æ£€æŸ¥å®ä½“æ˜¯å¦åœ¨å­å›¾ä¸­
        subgraph_entity_names = {node['name'] for node in subgraph['nodes']}

        valid_entities = [e for e in answer_entities if e in subgraph_entity_names]
        invalid_entities = [e for e in answer_entities if e not in subgraph_entity_names]

        # 3. æ£€æŸ¥å…³ç³»é™ˆè¿°
        relation_claims = self._extract_relation_claims(generated_answer)
        verified_claims = self._verify_claims_with_subgraph(relation_claims, subgraph)

        # 4. è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
        entity_consistency = len(valid_entities) / len(answer_entities) if answer_entities else 0
        claim_consistency = sum(1 for c in verified_claims if c['verified']) / len(
            verified_claims) if verified_claims else 0

        overall_score = 0.5 * entity_consistency + 0.5 * claim_consistency

        print(f"éªŒè¯ç»“æœ:")
        print(f"  - å®ä½“ä¸€è‡´æ€§: {entity_consistency:.2%}")
        print(f"  - å…³ç³»ä¸€è‡´æ€§: {claim_consistency:.2%}")
        print(f"  - æ€»ä½“ä¸€è‡´æ€§: {overall_score:.2%}\n")

        return {
            'overall_score': overall_score,
            'entity_consistency': entity_consistency,
            'claim_consistency': claim_consistency,
            'valid_entities': valid_entities,
            'invalid_entities': invalid_entities,
            'verified_claims': verified_claims
        }

    def _extract_entities_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        entities = []
        try:
            with self.driver.session() as session:
                result = session.run("""
                    MATCH (n)
                    WHERE $text_content CONTAINS n.name
                    RETURN DISTINCT n.name as name
                """, text_content=text)
                entities = [record['name'] for record in result]
        except Exception as e:
            print(f"âš ï¸  æå–å®ä½“å¤±è´¥: {e}")

        return entities

    def _extract_relation_claims(self, text: str) -> List[str]:
        """æå–å…³ç³»é™ˆè¿°"""
        # ç®€åŒ–ç‰ˆæœ¬:æå–å¥å­
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        return sentences

    def _verify_claims_with_subgraph(self, claims: List[str],
                                     subgraph: Dict) -> List[Dict]:
        """éªŒè¯é™ˆè¿°ä¸å­å›¾çš„ä¸€è‡´æ€§"""
        verified = []

        for claim in claims:
            # æ£€æŸ¥é™ˆè¿°ä¸­æ˜¯å¦åŒ…å«å­å›¾çš„è·¯å¾„
            is_verified = False
            supporting_path = None

            for path in subgraph['paths']:
                # ç®€å•æ£€æŸ¥:å¦‚æœé™ˆè¿°åŒ…å«è·¯å¾„ä¸­çš„å…³é”®å®ä½“
                path_entities = [node['name'] for node in path['nodes']]
                if any(entity in claim for entity in path_entities[:2]):
                    is_verified = True
                    supporting_path = path['description']
                    break

            verified.append({
                'claim': claim,
                'verified': is_verified,
                'supporting_path': supporting_path
            })

        return verified

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _call_deepseek(self, prompt: str, max_tokens: int = 1000,
                       temperature: float = 0) -> str:
        """è°ƒç”¨DeepSeek API"""
        if not self.use_llm:
            raise Exception("DeepSeek API æœªé…ç½®")

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.deepseek_api_key}"
        }

        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": temperature,
            "max_tokens": max_tokens
        }

        response = requests.post(self.deepseek_api_url, headers=headers,
                                 json=payload, timeout=60)
        response.raise_for_status()

        response_data = response.json()
        return response_data['choices'][0]['message']['content']