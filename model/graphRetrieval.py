import json
import re
from typing import List, Dict, Any, Tuple
from neo4j import GraphDatabase
import requests
import os


class KnowledgeGraphRetrieval:
    """çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸æ¨ç†ç³»ç»Ÿ"""

    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str,
                 deepseek_api_key: str = None):
        """
        åˆå§‹åŒ–
        Args:
            neo4j_uri: Neo4jæ•°æ®åº“URI
            neo4j_user: ç”¨æˆ·å
            neo4j_password: å¯†ç 
            deepseek_api_key: DeepSeek APIå¯†é’¥
        """
        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))
        self.deepseek_api_key = deepseek_api_key or os.getenv('DEEPSEEK_API_KEY')
        self.deepseek_api_url = "https://api.deepseek.com/v1/chat/completions"

    def close(self):
        """å…³é—­è¿æ¥"""
        self.driver.close()

    # ========== 1. å›¾æ£€ç´¢æ¨¡å— ==========

    def retrieve_relevant_subgraph(self, query: str, max_depth: int = 2,
                                   top_k: int = 10) -> Dict[str, Any]:
        """
        æ£€ç´¢ç›¸å…³å­å›¾
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            max_depth: æœ€å¤§æ£€ç´¢æ·±åº¦
            top_k: è¿”å›top-kä¸ªæœ€ç›¸å…³çš„è·¯å¾„
        Returns:
            å­å›¾æ•°æ®
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ” å¼€å§‹æ£€ç´¢ç›¸å…³å­å›¾")
        print(f"æŸ¥è¯¢: {query}")
        print(f"{'=' * 60}\n")

        # 1. æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“
        entities = self._extract_entities_from_query(query)
        print(f"âœ“ æå–åˆ°å…³é”®å®ä½“: {entities}\n")

        # 2. åœ¨å›¾è°±ä¸­æŸ¥æ‰¾åŒ¹é…çš„èŠ‚ç‚¹
        matched_nodes = self._find_matching_nodes(entities)
        print(f"âœ“ åŒ¹é…åˆ° {len(matched_nodes)} ä¸ªå›¾è°±èŠ‚ç‚¹\n")

        if not matched_nodes:
            print("âœ— æœªæ‰¾åˆ°åŒ¹é…èŠ‚ç‚¹\n")
            return {"nodes": [], "relationships": [], "paths": []}

        # 3. æ‰©å±•å­å›¾(BFS)
        subgraph = self._expand_subgraph(matched_nodes, max_depth, top_k)
        print(f"âœ“ æ‰©å±•å­å›¾å®Œæˆ:")
        print(f"  - èŠ‚ç‚¹æ•°: {len(subgraph['nodes'])}")
        print(f"  - å…³ç³»æ•°: {len(subgraph['relationships'])}")
        print(f"  - è·¯å¾„æ•°: {len(subgraph['paths'])}\n")

        return subgraph

    def _extract_entities_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®å®ä½“"""
        prompt = f"""ä»ä»¥ä¸‹åŒ»ç–—é—®é¢˜ä¸­æå–å…³é”®å®ä½“(ç–¾ç—…ã€æ²»ç–—ã€è¯ç‰©ã€æ£€æŸ¥ç­‰)ã€‚

é—®é¢˜: {query}

åªè¿”å›JSONæ•°ç»„,æ ¼å¼: ["å®ä½“1", "å®ä½“2", ...]

ç¤ºä¾‹:
é—®é¢˜: å¿ƒè„éª¤åœåº”è¯¥å¦‚ä½•æ€¥æ•‘?
è¾“å‡º: ["å¿ƒè„éª¤åœ", "æ€¥æ•‘"]
"""

        try:
            response = self._call_deepseek(prompt, max_tokens=200, temperature=0)
            response = response.strip()

            # æ¸…ç†markdown
            response = re.sub(r'```json\s*', '', response)
            response = re.sub(r'```\s*', '', response)

            entities = json.loads(response)
            return entities if isinstance(entities, list) else []
        except:
            # å¦‚æœå¤±è´¥,ä½¿ç”¨ç®€å•çš„å…³é”®è¯æå–
            keywords = []
            with self.driver.session() as session:
                # æŸ¥æ‰¾æŸ¥è¯¢ä¸­æåˆ°çš„æ‰€æœ‰èŠ‚ç‚¹åç§°
                result = session.run("""
                    MATCH (n)
                    WHERE $query CONTAINS n.name
                    RETURN DISTINCT n.name as name
                    LIMIT 10
                """, query=query)
                keywords = [record['name'] for record in result]

            return keywords if keywords else [query]

    def _find_matching_nodes(self, entities: List[str]) -> List[Dict]:
        """æŸ¥æ‰¾åŒ¹é…çš„å›¾è°±èŠ‚ç‚¹"""
        matched = []

        with self.driver.session() as session:
            for entity in entities:
                # æ¨¡ç³ŠåŒ¹é…
                result = session.run("""
                    MATCH (n)
                    WHERE n.name CONTAINS $entity
                    RETURN id(n) as node_id, 
                           labels(n)[0] as type,
                           n.name as name,
                           properties(n) as properties
                    LIMIT 5
                """, entity=entity)

                for record in result:
                    matched.append({
                        'id': record['node_id'],
                        'type': record['type'],
                        'name': record['name'],
                        'properties': dict(record['properties'])
                    })

        return matched

    def _expand_subgraph(self, seed_nodes: List[Dict], max_depth: int,
                         top_k: int) -> Dict[str, Any]:
        """æ‰©å±•å­å›¾"""
        node_ids = [node['id'] for node in seed_nodes]

        with self.driver.session() as session:
            # æŸ¥è¯¢å­å›¾è·¯å¾„
            result = session.run(f"""
                MATCH path = (start)-[*1..{max_depth}]-(end)
                WHERE id(start) IN $node_ids
                WITH path, 
                     length(path) as path_length,
                     [rel in relationships(path) | type(rel)] as rel_types
                RETURN path,
                       [node in nodes(path) | {{
                           id: id(node),
                           type: labels(node)[0],
                           name: node.name,
                           properties: properties(node)
                       }}] as nodes,
                       [rel in relationships(path) | {{
                           type: type(rel),
                           properties: properties(rel)
                       }}] as relationships,
                       path_length,
                       rel_types
                ORDER BY path_length ASC
                LIMIT $top_k
            """, node_ids=node_ids, top_k=top_k)

            # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
            all_nodes = {}
            all_relationships = []
            all_paths = []

            for record in result:
                nodes = record['nodes']
                rels = record['relationships']

                # æ”¶é›†èŠ‚ç‚¹
                for node in nodes:
                    node_id = node['id']
                    if node_id not in all_nodes:
                        all_nodes[node_id] = node

                # æ”¶é›†å…³ç³»
                for i, rel in enumerate(rels):
                    rel_data = {
                        'from': nodes[i]['id'],
                        'from_name': nodes[i]['name'],
                        'to': nodes[i + 1]['id'],
                        'to_name': nodes[i + 1]['name'],
                        'type': rel['type'],
                        'properties': rel['properties']
                    }
                    all_relationships.append(rel_data)

                # è®°å½•è·¯å¾„
                path_desc = ' -> '.join([
                                            f"{nodes[i]['name']}[{rels[i]['type']}]"
                                            for i in range(len(rels))
                                        ] + [nodes[-1]['name']])

                all_paths.append({
                    'nodes': nodes,
                    'relationships': rels,
                    'description': path_desc,
                    'length': record['path_length']
                })

            return {
                'nodes': list(all_nodes.values()),
                'relationships': all_relationships,
                'paths': all_paths
            }

    # ========== 2. è‡ªä¸€è‡´æ€§æ£€ç´¢ ==========

    def self_consistency_retrieval(self, query: str, num_samples: int = 3) -> Dict[str, Any]:
        """
        è‡ªä¸€è‡´æ€§æ£€ç´¢: å¤šæ¬¡æ£€ç´¢å–ä¸€è‡´ç»“æœ
        Args:
            query: æŸ¥è¯¢
            num_samples: é‡‡æ ·æ¬¡æ•°
        Returns:
            ä¸€è‡´æ€§æ£€ç´¢ç»“æœ
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ”„ è‡ªä¸€è‡´æ€§æ£€ç´¢ (é‡‡æ ·{num_samples}æ¬¡)")
        print(f"{'=' * 60}\n")

        # 1. å¤šæ¬¡æ£€ç´¢,ä½¿ç”¨ä¸åŒçš„éšæœºæ€§
        all_subgraphs = []
        for i in range(num_samples):
            print(f"ç¬¬ {i + 1}/{num_samples} æ¬¡æ£€ç´¢...")
            subgraph = self.retrieve_relevant_subgraph(query, max_depth=2, top_k=8)
            all_subgraphs.append(subgraph)

        print(f"\nâœ“ å®Œæˆ {num_samples} æ¬¡æ£€ç´¢\n")

        # 2. ç»Ÿè®¡ä¸€è‡´æ€§
        node_counter = {}  # èŠ‚ç‚¹å‡ºç°æ¬¡æ•°
        path_counter = {}  # è·¯å¾„å‡ºç°æ¬¡æ•°

        for subgraph in all_subgraphs:
            # ç»Ÿè®¡èŠ‚ç‚¹
            for node in subgraph['nodes']:
                key = (node['type'], node['name'])
                node_counter[key] = node_counter.get(key, 0) + 1

            # ç»Ÿè®¡è·¯å¾„æ¨¡å¼
            for path in subgraph['paths']:
                path_pattern = ' -> '.join([
                                               f"{path['nodes'][i]['name']}[{path['relationships'][i]['type']}]"
                                               for i in range(len(path['relationships']))
                                           ] + [path['nodes'][-1]['name']])

                path_counter[path_pattern] = path_counter.get(path_pattern, 0) + 1

        # 3. ç­›é€‰é«˜ä¸€è‡´æ€§çš„ç»“æœ
        threshold = num_samples // 2 + 1  # è¶…è¿‡åŠæ•°

        consistent_nodes = [
            {'type': k[0], 'name': k[1], 'consistency': v / num_samples}
            for k, v in node_counter.items() if v >= threshold
        ]

        consistent_paths = [
            {'pattern': k, 'consistency': v / num_samples}
            for k, v in path_counter.items() if v >= threshold
        ]

        print(f"ä¸€è‡´æ€§åˆ†æç»“æœ:")
        print(f"  - é«˜ä¸€è‡´æ€§èŠ‚ç‚¹: {len(consistent_nodes)} ä¸ª")
        print(f"  - é«˜ä¸€è‡´æ€§è·¯å¾„: {len(consistent_paths)} ä¸ª\n")

        return {
            'query': query,
            'num_samples': num_samples,
            'consistent_nodes': consistent_nodes,
            'consistent_paths': consistent_paths,
            'all_subgraphs': all_subgraphs
        }

    # ========== 3. åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ ==========

    def controlled_generation_with_subgraph(self, query: str,
                                            use_consistency: bool = True) -> str:
        """
        åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ
        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_consistency: æ˜¯å¦ä½¿ç”¨è‡ªä¸€è‡´æ€§æ£€ç´¢
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ¯ åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ")
        print(f"{'=' * 60}\n")

        # 1. æ£€ç´¢å­å›¾
        if use_consistency:
            consistency_result = self.self_consistency_retrieval(query, num_samples=3)
            # ä½¿ç”¨æœ€ä¸€è‡´çš„å­å›¾
            subgraph = consistency_result['all_subgraphs'][0]
            consistent_info = f"""
ä¸€è‡´æ€§åˆ†æ:
- é«˜ä¸€è‡´æ€§èŠ‚ç‚¹: {len(consistency_result['consistent_nodes'])} ä¸ª
- é«˜ä¸€è‡´æ€§è·¯å¾„: {len(consistency_result['consistent_paths'])} ä¸ª

æ ¸å¿ƒèŠ‚ç‚¹: {', '.join([n['name'] for n in consistency_result['consistent_nodes'][:5]])}
"""
        else:
            subgraph = self.retrieve_relevant_subgraph(query, max_depth=2, top_k=10)
            consistent_info = ""

        # 2. æ„å»ºç»“æ„åŒ–çŸ¥è¯†
        structured_knowledge = self._format_subgraph_for_generation(subgraph)

        # 3. åŸºäºå­å›¾ç”Ÿæˆç­”æ¡ˆ
        answer = self._generate_with_constraints(query, structured_knowledge, consistent_info)

        return answer

    def _format_subgraph_for_generation(self, subgraph: Dict) -> str:
        """å°†å­å›¾æ ¼å¼åŒ–ä¸ºç»“æ„åŒ–çŸ¥è¯†"""
        knowledge_parts = []

        # æ ¼å¼åŒ–èŠ‚ç‚¹ä¿¡æ¯
        knowledge_parts.append("ã€ç›¸å…³åŒ»ç–—å®ä½“ã€‘")

        # æŒ‰ç±»å‹åˆ†ç»„
        nodes_by_type = {}
        for node in subgraph['nodes']:
            node_type = node['type']
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node)

        for node_type, nodes in nodes_by_type.items():
            knowledge_parts.append(f"\n{node_type}:")
            for node in nodes[:5]:  # é™åˆ¶æ•°é‡
                props = node.get('properties', {})
                prop_str = ', '.join([f"{k}:{v}" for k, v in props.items()
                                      if k not in ['id', 'name']])
                if prop_str:
                    knowledge_parts.append(f"  - {node['name']} ({prop_str})")
                else:
                    knowledge_parts.append(f"  - {node['name']}")

        # æ ¼å¼åŒ–å…³ç³»å’Œè·¯å¾„
        knowledge_parts.append("\nã€åŒ»ç–—çŸ¥è¯†å…³è”ã€‘")
        for path in subgraph['paths'][:5]:  # é™åˆ¶è·¯å¾„æ•°é‡
            knowledge_parts.append(f"  {path['description']}")

        return '\n'.join(knowledge_parts)

    def _generate_with_constraints(self, query: str, structured_knowledge: str,
                                   consistency_info: str) -> str:
        """åŸºäºçº¦æŸç”Ÿæˆç­”æ¡ˆ"""
        print("ğŸ“ ç”Ÿæˆç­”æ¡ˆ...\n")

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ã€é‡è¦çº¦æŸã€‘
1. å¿…é¡»åŸºäºæä¾›çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”
2. ä¸è¦ç¼–é€ çŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰çš„ä¿¡æ¯
3. å¦‚æœçŸ¥è¯†å›¾è°±ä¿¡æ¯ä¸è¶³,æ˜ç¡®è¯´æ˜
4. æŒ‰ç…§"ç–¾ç—…è¯†åˆ« -> æ²»ç–—æªæ–½ -> ç”¨è¯æŒ‡å¯¼ -> ç›‘æµ‹è¦ç‚¹"çš„ç»“æ„ç»„ç»‡ç­”æ¡ˆ
5. å¼•ç”¨å…·ä½“çš„å®ä½“å’Œå…³ç³»

{consistency_info}

ã€çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‘
{structured_knowledge}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
- ç»“æ„æ¸…æ™°,åˆ†ç‚¹ä½œç­”
- å¼•ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„å…·ä½“ä¿¡æ¯
- æ ‡æ³¨ä¿¡æ¯æ¥æº(å¦‚"æ ¹æ®çŸ¥è¯†å›¾è°±...")
- å¦‚æœ‰å±æ€§ä¿¡æ¯(å‰‚é‡ã€æ—¶æœºç­‰),åŠ¡å¿…åŒ…å«

è¯·å›ç­”:
"""

        response = self._call_deepseek(prompt, max_tokens=1000, temperature=0.3)

        return response

    # ========== 4. å¤šè·³æ¨ç† ==========

    def multi_hop_reasoning(self, query: str, max_hops: int = 3) -> Dict[str, Any]:
        """
        å¤šè·³æ¨ç†
        Args:
            query: æŸ¥è¯¢
            max_hops: æœ€å¤§æ¨ç†è·³æ•°
        Returns:
            æ¨ç†é“¾
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ§  å¤šè·³æ¨ç† (æœ€å¤§{max_hops}è·³)")
        print(f"{'=' * 60}\n")

        # 1. æå–èµ·å§‹å®ä½“å’Œç›®æ ‡
        entities = self._extract_entities_from_query(query)
        if len(entities) < 2:
            print("âœ— éœ€è¦è‡³å°‘2ä¸ªå®ä½“è¿›è¡Œå¤šè·³æ¨ç†\n")
            return None

        start_entity = entities[0]
        end_entity = entities[-1]

        print(f"èµ·å§‹å®ä½“: {start_entity}")
        print(f"ç›®æ ‡å®ä½“: {end_entity}\n")

        # 2. æŸ¥æ‰¾æ¨ç†è·¯å¾„
        reasoning_paths = self._find_reasoning_paths(start_entity, end_entity, max_hops)

        print(f"âœ“ æ‰¾åˆ° {len(reasoning_paths)} æ¡æ¨ç†è·¯å¾„\n")

        # 3. è¯„åˆ†å’Œæ’åº
        scored_paths = self._score_reasoning_paths(reasoning_paths)

        return {
            'start': start_entity,
            'end': end_entity,
            'paths': scored_paths
        }

    def _find_reasoning_paths(self, start: str, end: str, max_hops: int) -> List[Dict]:
        """æŸ¥æ‰¾æ¨ç†è·¯å¾„"""
        with self.driver.session() as session:
            result = session.run(f"""
                MATCH path = (start)-[*1..{max_hops}]-(end)
                WHERE start.name CONTAINS $start AND end.name CONTAINS $end
                WITH path,
                     [node in nodes(path) | node.name] as node_names,
                     [rel in relationships(path) | type(rel)] as rel_types,
                     length(path) as hops
                RETURN node_names, rel_types, hops
                ORDER BY hops ASC
                LIMIT 10
            """, start=start, end=end)

            paths = []
            for record in result:
                paths.append({
                    'nodes': record['node_names'],
                    'relations': record['rel_types'],
                    'hops': record['hops']
                })

            return paths

    def _score_reasoning_paths(self, paths: List[Dict]) -> List[Dict]:
        """è¯„åˆ†æ¨ç†è·¯å¾„"""
        for path in paths:
            # è¯„åˆ†å› ç´ :
            # 1. è·¯å¾„é•¿åº¦(è¶ŠçŸ­è¶Šå¥½)
            length_score = 1.0 / (path['hops'] + 1)

            # 2. å…³ç³»ç±»å‹é‡è¦æ€§
            important_rels = ['éœ€è¦æ²»ç–—', 'ä½¿ç”¨è¯ç‰©', 'éœ€è¦æ£€æŸ¥']
            rel_score = sum(1 for r in path['relations'] if r in important_rels) / len(path['relations'])

            # ç»¼åˆè¯„åˆ†
            path['score'] = 0.6 * length_score + 0.4 * rel_score

        # æ’åº
        paths.sort(key=lambda x: x['score'], reverse=True)

        return paths

    # ========== 5. å­å›¾éªŒè¯ ==========

    def validate_generation_with_subgraph(self, generated_answer: str,
                                          subgraph: Dict) -> Dict[str, Any]:
        """
        éªŒè¯ç”Ÿæˆå†…å®¹æ˜¯å¦ä¸å­å›¾ä¸€è‡´
        Args:
            generated_answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            subgraph: å‚è€ƒå­å›¾
        Returns:
            éªŒè¯ç»“æœ
        """
        print(f"\n{'=' * 60}")
        print(f"âœ… éªŒè¯ç”Ÿæˆå†…å®¹")
        print(f"{'=' * 60}\n")

        # 1. æå–ç­”æ¡ˆä¸­çš„å®ä½“
        answer_entities = self._extract_entities_from_text(generated_answer)

        # 2. æ£€æŸ¥å®ä½“æ˜¯å¦åœ¨å­å›¾ä¸­
        subgraph_entity_names = {node['name'] for node in subgraph['nodes']}

        valid_entities = [e for e in answer_entities if e in subgraph_entity_names]
        invalid_entities = [e for e in answer_entities if e not in subgraph_entity_names]

        # 3. æ£€æŸ¥å…³ç³»é™ˆè¿°
        relation_claims = self._extract_relation_claims(generated_answer)
        verified_claims = self._verify_claims_with_subgraph(relation_claims, subgraph)

        # 4. è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
        entity_consistency = len(valid_entities) / len(answer_entities) if answer_entities else 0
        claim_consistency = sum(1 for c in verified_claims if c['verified']) / len(
            verified_claims) if verified_claims else 0

        overall_score = 0.5 * entity_consistency + 0.5 * claim_consistency

        print(f"éªŒè¯ç»“æœ:")
        print(f"  - å®ä½“ä¸€è‡´æ€§: {entity_consistency:.2%}")
        print(f"  - å…³ç³»ä¸€è‡´æ€§: {claim_consistency:.2%}")
        print(f"  - æ€»ä½“ä¸€è‡´æ€§: {overall_score:.2%}\n")

        return {
            'overall_score': overall_score,
            'entity_consistency': entity_consistency,
            'claim_consistency': claim_consistency,
            'valid_entities': valid_entities,
            'invalid_entities': invalid_entities,
            'verified_claims': verified_claims
        }

    def _extract_entities_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        # ç®€åŒ–ç‰ˆæœ¬:åŒ¹é…å›¾è°±ä¸­çš„èŠ‚ç‚¹åç§°
        entities = []
        with self.driver.session() as session:
            result = session.run("""
                MATCH (n)
                WHERE $text CONTAINS n.name
                RETURN DISTINCT n.name as name
            """, text=text)
            entities = [record['name'] for record in result]

        return entities

    def _extract_relation_claims(self, text: str) -> List[str]:
        """æå–å…³ç³»é™ˆè¿°"""
        # ç®€åŒ–ç‰ˆæœ¬:æå–å¥å­
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        return sentences

    def _verify_claims_with_subgraph(self, claims: List[str],
                                     subgraph: Dict) -> List[Dict]:
        """éªŒè¯é™ˆè¿°ä¸å­å›¾çš„ä¸€è‡´æ€§"""
        verified = []

        for claim in claims:
            # æ£€æŸ¥é™ˆè¿°ä¸­æ˜¯å¦åŒ…å«å­å›¾çš„è·¯å¾„
            is_verified = False
            supporting_path = None

            for path in subgraph['paths']:
                # ç®€å•æ£€æŸ¥:å¦‚æœé™ˆè¿°åŒ…å«è·¯å¾„ä¸­çš„å…³é”®å®ä½“
                path_entities = [node['name'] for node in path['nodes']]
                if any(entity in claim for entity in path_entities[:2]):
                    is_verified = True
                    supporting_path = path['description']
                    break

            verified.append({
                'claim': claim,
                'verified': is_verified,
                'supporting_path': supporting_path
            })

        return verified

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _call_deepseek(self, prompt: str, max_tokens: int = 1000,
                       temperature: float = 0) -> str:
        """è°ƒç”¨DeepSeek API"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.deepseek_api_key}"
        }

        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": temperature,
            "max_tokens": max_tokens
        }

        response = requests.post(self.deepseek_api_url, headers=headers,
                                 json=payload, timeout=60)
        response.raise_for_status()

        response_data = response.json()
        return response_data['choices'][0]['message']['content']


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

def main():
    """ä¸»å‡½æ•°"""

    # é…ç½®
    NEO4J_URI = "bolt://localhost:7687"
    NEO4J_USER = "neo4j"
    NEO4J_PASSWORD = "your_password"
    DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY') or "your_api_key"

    # åˆ›å»ºæ£€ç´¢ç³»ç»Ÿ
    retrieval = KnowledgeGraphRetrieval(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD,
                                        DEEPSEEK_API_KEY)

    try:
        # ========== ç¤ºä¾‹1: åŸºæœ¬å­å›¾æ£€ç´¢ ==========
        print("\n" + "=" * 60)
        print("ç¤ºä¾‹1: åŸºæœ¬å­å›¾æ£€ç´¢")
        print("=" * 60)

        query1 = "å¿ƒè„éª¤åœåº”è¯¥å¦‚ä½•æ€¥æ•‘æ²»ç–—?"
        subgraph1 = retrieval.retrieve_relevant_subgraph(query1, max_depth=2, top_k=10)

        print("æ£€ç´¢åˆ°çš„å…³é”®è·¯å¾„:")
        for i, path in enumerate(subgraph1['paths'][:3], 1):
            print(f"  {i}. {path['description']}")

        # ========== ç¤ºä¾‹2: è‡ªä¸€è‡´æ€§æ£€ç´¢ ==========
        print("\n" + "=" * 60)
        print("ç¤ºä¾‹2: è‡ªä¸€è‡´æ€§æ£€ç´¢")
        print("=" * 60)

        query2 = "æ€¥æ€§å† è„‰ç»¼åˆå¾éœ€è¦å“ªäº›æ²»ç–—?"
        consistency_result = retrieval.self_consistency_retrieval(query2, num_samples=3)

        print("é«˜ä¸€è‡´æ€§èŠ‚ç‚¹:")
        for node in consistency_result['consistent_nodes'][:5]:
            print(f"  - {node['name']} (ä¸€è‡´æ€§: {node['consistency']:.0%})")

        print("\né«˜ä¸€è‡´æ€§è·¯å¾„:")
        for path in consistency_result['consistent_paths'][:3]:
            print(f"  - {path['pattern']}")
            print(f"    ä¸€è‡´æ€§: {path['consistency']:.0%}")

        # ========== ç¤ºä¾‹3: åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ ==========
        print("\n" + "=" * 60)
        print("ç¤ºä¾‹3: åŸºäºå­å›¾çš„æ§åˆ¶ç”Ÿæˆ")
        print("=" * 60)

        query3 = "å¿ƒè„éª¤åœçš„å®Œæ•´æ€¥æ•‘æµç¨‹æ˜¯ä»€ä¹ˆ?"
        answer = retrieval.controlled_generation_with_subgraph(query3, use_consistency=True)

        print("ç”Ÿæˆçš„ç­”æ¡ˆ:")
        print("-" * 60)
        print(answer)
        print("-" * 60)

        # ========== ç¤ºä¾‹4: éªŒè¯ç”Ÿæˆå†…å®¹ ==========
        print("\n" + "=" * 60)
        print("ç¤ºä¾‹4: éªŒè¯ç”Ÿæˆå†…å®¹")
        print("=" * 60)

        subgraph3 = retrieval.retrieve_relevant_subgraph(query3)
        validation = retrieval.validate_generation_with_subgraph(answer, subgraph3)

        print(f"\næ€»ä½“ä¸€è‡´æ€§å¾—åˆ†: {validation['overall_score']:.2%}")
        print(f"\næœ‰æ•ˆå®ä½“: {', '.join(validation['valid_entities'][:5])}")
        if validation['invalid_entities']:
            print(f"æ— æ•ˆå®ä½“: {', '.join(validation['invalid_entities'])}")

        print("\nå·²éªŒè¯çš„é™ˆè¿°:")
        for claim in validation['verified_claims'][:3]:
            status = "âœ“" if claim['verified'] else "âœ—"
            print(f"  {status} {claim['claim'][:50]}...")

        # ========== ç¤ºä¾‹5: å¤šè·³æ¨ç† ==========
        print("\n" + "=" * 60)
        print("ç¤ºä¾‹5: å¤šè·³æ¨ç†")
        print("=" * 60)

        query5 = "å¿ƒè„éª¤åœå’Œè‚¾ä¸Šè…ºç´ ä¹‹é—´æœ‰ä»€ä¹ˆå…³ç³»?"
        reasoning = retrieval.multi_hop_reasoning(query5, max_hops=3)

        if reasoning and reasoning['paths']:
            print(f"\næ‰¾åˆ°ä» {reasoning['start']} åˆ° {reasoning['end']} çš„æ¨ç†è·¯å¾„:\n")
            for i, path in enumerate(reasoning['paths'][:3], 1):
                print(f"è·¯å¾„{i} (å¾—åˆ†: {path['score']:.3f}, {path['hops']}è·³):")
                for j in range(len(path['relations'])):
                    print(f"  {path['nodes'][j]} --[{path['relations'][j]}]--> {path['nodes'][j + 1]}")
                print()

    finally:
        retrieval.close()
        print("\næ•°æ®åº“è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    main()